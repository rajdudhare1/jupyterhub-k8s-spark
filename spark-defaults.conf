spark.master=k8s://https://kubernetes.default.svc:443 # Replace this address if you have anything different
spark.driver.bindAddress=0.0.0.0 # Bind driver with single-user pod of JupyterHub
spark.driver.port=7077 # Bind driver port with single-user pod of JupyterHub
spark.blockManager.port=6060 # Bind block manager port with single-user pod of JupyterHub
spark.kubernetes.namespace=default # Add your namespace 
spark.executor.instances=2 # Default number of executors
spark.executor.cores=1 # Default number of executors cores
spark.executor.memory=1g # Default executor memory
spark.driver.memory=1g # Default driver memory
## Use your custom image for spark as you may have dependecies in it. 
## https://spark.apache.org/docs/latest/running-on-kubernetes.html#docker-images
## Also provided sample Dockerfile that will work for this example
spark.kubernetes.container.image=<your.registry.example.com>/<your-repository>/spark:<version>
spark.kubernetes.container.image.pullSecrets=<your-repository-creds-here> # Provide image secrets
spark.kubernetes.container.image.pullPolicy=Always # Better if you are modifying same tag always
## Ensure pods are cleaned up once you are done with execution
spark.kubernetes.cleanup=true
spark.kubernetes.cleanup.time=5s
## Provide service account we created earlier
spark.kubernetes.authenticate.driver.serviceAccountName=spark
## Ivy path for jars
spark.jars.ivy=/tmp/.ivy
## S3 specific configuration tweak values as per your requirement do uncommment following lines
# spark.hadoop.fs.defaultFS=s3a://<BUCKET>/
# spark.hadoop.fs.s3a.committer.magic.enabled=true
# spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
# spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.hadoop.fs.s3a.connection.maximum=10000
# spark.hadoop.fs.s3.maxConnections=10000
# spark.hadoop.fs.s3a.connection.establish.timeout=50000
# spark.hadoop.fs.s3a.socket.recv.buffer=8192000
# spark.hadoop.fs.s3a.readahead.range=32M
# spark.hadoop.fs.s3a.path.style.access=true
# spark.hadoop.com.amazonaws.services.s3.enableV4=true

## Its not recommanded to put secrets in configuration file, instead use environment variables from secrets
# ref https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data
# spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=<K8S secret name>:key
# spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=<K8S secret name>:key
# spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=<K8S secret name>:key
# spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=<K8S secret name>:key

## Still, if you want to use it then uncommment following lines
# spark.hadoop.fs.s3a.access.key=<S3 ACCESS KEY>
# spark.hadoop.fs.s3a.secret.key=<S3 SECRET KEY>
# spark.hadoop.fs.s3a.endpoint=<> # Add your s3 endpoint here

## Put these files over S3 or GCS and provide path below format, for S3 - s3a:// and for GCS - gc://
# spark.kubernetes.driver.podTemplateFile=s3a://<BUCKET>/<DIRECTORY>/pod_template.yaml
# spark.kubernetes.executor.podTemplateFile=s3a://<BUCKET>/<DIRECTORY>/pod_template.yaml


## Log your spark jobs by uncommenting below lines 
# spark.eventLog.enabled=true
# spark.eventLog.dir=s3a://<BUCKET>/spark-events/

## To mount NFS to specific location on executors uncomment below lines 
# spark.kubernetes.executor.volumes.nfs.images.options.server=<NFS SERVER ADDRESS>
# spark.kubernetes.executor.volumes.nfs.images.options.path=/ # Path of NFS Server
# spark.kubernetes.executor.volumes.nfs.images.mount.path=/mnt # Mount path inside executor pod 
